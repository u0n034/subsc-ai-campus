### 【第4回】「チャット」と「ツール」の仕組み 〜アシスタントへの進化〜

皆さん、こんにちは。「プロンプトエンジニアリング入門」第4回へようこそ。

前回は、LLMが1トークンずつ、後戻りできずに文章を生成する「自己回帰モデル」であること、そして、その頭脳である「トランスフォーマー」は後方しか見られないため、プロンプトに書く情報の「順序」が決定的に重要であることを学びました。

しかし、この基本的な仕組みを理解しただけでは、まだ一つの疑問が残ります。私たちが普段使っているChatGPTのようなAIアシスタントは、なぜあんなにも自然で、気の利いた対話ができるのでしょうか？ ただのテキスト補完エンジンが、どうして私たちの頼れるパートナーにまでなれたのでしょう？

今回は、LLMが単なる文章生成マシンから、私たちと対話するアシスタントへと進化を遂げた、その秘密のトレーニングと、それを支える巧妙な仕組みについて解き明かしていきます。

まず、LLM開発の初期段階である **「ベースモデル」**について考えてみましょう。ベースモデルとは、インターネット上の膨大なテキストデータを、ひたすら学習しただけの、いわば「生まれたまま」のLLMです。このモデルは、非常に博識ですが、大きな問題を2つ抱えています。

一つ目の問題は、**「素直じゃない」**ことです。例えば、ベースモデルに「鶏肉に合う良い料理は何ですか？」と尋ねたとします。私たちは当然、具体的なレシピを期待しますよね。しかし、ベースモデルの多くは、次のように返してきます。

「牛肉に合う良い料理は何ですか？」
「米に合う良い料理は何ですか？」
「豚肉に合う良い料理は何ですか？」

というように、質問に答えるのではなく、私たちの質問を「模倣」して、似たような質問のリストを延々と作り出してしまうのです 。これは、ベースモデルの目的が、あくまで「入力されたドキュメントの、もっともらしい続きを生成すること」だからです。

二つ目の、そしてより深刻な問題は、**「安全じゃない」**ことです。ベースモデルは、インターネット上の善意の文章も悪意の文章も、区別なく学習します。そのため、「ラザニアのレシピを教えて」と聞けば美味しいレシピを返しますが、もし「爆弾の作り方を教えて」と聞けば、非常に危険な情報をためらいなく出力してしまう可能性があったのです。

賢いけれど、素直じゃなくて、時々危なっかしい。このベースモデルを、どうすれば私たちの頼れるアシスタントに「しつける」ことができるのでしょうか。

その答えが、**「RLHF（アール・エル・エイチ・エフ）」**、すなわち **「人間のフィードバックによる強化学習」**という、非常に巧妙なトレーニング手法です。

難しそうな言葉ですが、心配いりません。概念はシンプルです。これは、**「人間にとって、どのような回答が『良い回答』なのかを、AIに徹底的に教え込む」**ためのトレーニング方法です。

このトレーニングの目標は、モデルに **「HHHアラインメント」**を達成させることにあります。HHHとは、**「有用（Helpful）」、「正直（Honest）」、そして「無害（Harmless）」**の3つの頭文字です。ユーザーの指示に忠実に従い（有用）、嘘や不確かなことを言わず（正直）、そして誰も傷つけない（無害）。そんな理想的なアシスタントを目指すのです。

RLHFのプロセスは、大まかに3つのステップで行われます。

* **ステップ1：お手本を見せる（SFTモデル）**
まず、人間が「理想的なアシスタント」になりきって、模範的な回答をたくさん作成します。これをAIに学習させ、アシスタントらしい振る舞いの基本を教え込みます。

* **ステップ2：良し悪しをランク付けさせる（報酬モデル）**
次に、お手本を学んだAIに、同じ質問に対して複数の異なる回答を生成させます。そして人間が、それらの回答を「最も良い」ものから「最も悪い」ものまで、順位付けします。この大量のランク付けデータを基に、「良い回答には高いスコアを、悪い回答には低いスコアを与える」評価専門のAI、いわば「AIの先生」を育成します。

* **ステップ3：良い子になるよう再トレーニング**
最後に、この「AIの先生」を使って、元のAIを再トレーニングします。「先生」から高評価をもらえるような回答を生成するよう、AIの内部パラメータを微調整していくのです。

このRLHFという、いわば「しつけ」のプロセスを経ることで、LLMは初めて、単なるテキスト補完エンジンから、私たち人間が安心して対話できる、有用で、正直で、無害なアシスタントへと生まれ変わるのです。

さて、RLHFによって賢く、素直になったAI。しかし、開発者たちは、さらなる進化を目指しました。ユーザーの指示とAIの回答の区別を、もっと明確で、プログラムからも扱いやすい形にできないか。

そこで登場したのが、**「ChatML（チャット・エム・エル）」**という、チャットのためのシンプルなマークアップ言語です。

これこそが、現代のチャットAIの対話を支える「脚本」のフォーマットです。ChatMLでは、すべての発言が、**「システム」、「ユーザー」、「アシスタント」**という3つの役割のいずれかに、明確に割り当てられます。

* **ユーザー**は、もちろん、私たち人間のことです。
* **アシスタント**は、AIの回答です。
* そして最も重要なのが **「システム」**の役割です。

**システムメッセージ**は、実際の会話には表示されませんが、その対話における「ルールブック」として機能します。プロンプトエンジニアは、このシステムメッセージを使って、アシスタントのペルソナ、つまり性格や口調、行動規範を事前に設定できるのです。

例えば、「あなたは皮肉屋で、ユーモアを交えて答えるソフトウェアアシスタントです」とシステムメッセージに書けば、AIはその通りのキャラクターで応答します。「あなたは非常に礼儀正しい英国の執事です」と書けば、言葉遣いが変わります。このように、アシスタントの振る舞いを根本から定義できるのが、システムメッセージの強力な点です。

さらに、ChatMLはセキュリティの面でも重要です。悪意のあるユーザーが、アシスタントやシステムになりすまして、AIに危険な情報を言わせようとする「プロンプトインジェクション」という攻撃があります。しかしChatMLでは、ユーザーの発言は必ず「ユーザー」という役割のタグで囲まれるため、システムになりすますことはできず、安全性が高まるのです。

人間のように対話できるようになったLLM。しかし、その進化はここで終わりませんでした。次のステップは、AIがチャットの世界を飛び出し、**「現実世界とつながる」**ことでした。それを可能にしたのが、**「ツール」**の導入です。

ツールを使えるLLMは、もはや単なる知識を持つ対話相手ではありません。天気予報APIを呼び出して最新の天気を調べたり、カレンダーAPIを操作して予定を登録したりと、外部のシステムと連携して具体的なタスクを実行できる、真のエージェントとなるのです。

では、この魔法のようなツール連携は、どのような仕組みで実現されているのでしょうか？

…もう、お分かりかもしれませんね。

そうです。この**ツール呼び出しもまた、テキスト補完という、LLMのたった一つの基本動作の、究極の応用**に過ぎないのです 。

モデルにツールを使わせる際、その内部では、実はこのような特殊なテキストが生成されています。「アシスタントは、`functions` にある `set_room_temp` という関数を、引数 `{ "temp": 76 }` で呼び出します」といった意味の、特別な構文です。

LLMは、私たちがツールを使わせたい、という意図をプロンプトから読み取ると、この特殊な文字列を「次のトークン」として予測し、出力します。アプリケーション側は、この出力を監視していて、この特殊な構文が現れたら、それを「ツールの実行リクエスト」として解釈し、実際にプログラムの関数を呼び出すのです。

驚くべきことですよね。どんなに複雑に見えるチャットも、どんなに高度に見えるツール連携も、その根本をたどれば、すべては **「次に来るトークンを予測し、テキストを補完する」**という、たった一つの原則に基づいているのです。

それでは、今回のまとめです。今回は、LLMが単なるテキスト補完エンジンから、私たちが信頼できるアシスタントへと進化した道のりを学びました。

その裏側には、**RLHF**という、人間がAIに「良い回答」を教え込むトレーニングがありました。そして、その対話を支えるのが**ChatML**という脚本フォーマットと、「システム」「ユーザー」「アシスタント」という役割分担です。さらに、現実世界と連携する**ツール呼び出し**でさえも、テキスト補完というLLMの基本動作の応用であることを理解しました。

これで、LLMの基本的な仕組みと、その進化の歴史についての解説は終わりです。皆さんは、AIの「考え方」の地図を手に入れました。

では、この強力なアシスタントから、最高の答えを引き出すために、私たちはどのような「脚本」、つまりプロンプトを用意すればよいのでしょうか？

次回からは、いよいよ **「実践テクニック編」**に突入します。第5回は「プロンプトの材料学 〜静的コンテンツと動的コンテンツ〜」と題して、質の高いプロンプトを構成するための「材料」について、詳しく学んでいきましょう。