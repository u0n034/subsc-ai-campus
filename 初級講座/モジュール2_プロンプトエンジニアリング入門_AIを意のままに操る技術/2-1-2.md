### 【第2回】LLMが見る世界 〜トークンとコンテキストウィンドウ〜

皆さん、こんにちは。「プロンプトエンジニアリング入門」第2回へようこそ。

前回、私たちはLLM、大規模言語モデルの正体が、実は**「学習した膨大なテキストを模倣する、テキスト補完エンジンである」**という、この講座の最も重要な原則を学びました。

では、そのAIは、私たち人間と全く同じように「文字」を読み、「言葉」を理解しているのでしょうか？ 実は、ここに大きな誤解があります。そして、この誤解を解くことこそが、AIを意のままに操るプロンプトエンジニアになるための、次のステップなのです。

まず、私たち人間がどうやって文章を読んでいるか、少し考えてみましょう。

2つの奇妙な日本語の文章を例として見てみましょう。まず一つ目です。

「こんちには みさなん おんげき ですか？」

いかがでしょう？ 少し違和感はありますが、「こんにちは みなさん お元気ですか？」と、意味は読み取れたのではないでしょうか。

では次に、二つ目の文章を読んでみてください。

「こん にちはみ なさんお げんきで すか？」

…いかがですか？こちらは、一つ目よりもずっと読むのが難しく、ストレスを感じませんか？

この二つの文章の違いは、一つ目が単語の塊は保ったまま中の文字を入れ替えたのに対し、二つ目は文字の順番は正しいのに単語の区切りを無視している点にあります。このことからわかるように、私たち人間も、実は一文字一文字を厳密に読んでいるわけではなく、意味のある「単語」という塊（チャンク）としてテキストを認識しているのです。

そして、LLMもまた、人間と同じように、一文字ずつテキストを読んでいるわけではありません。しかし、その「塊」の作り方が、人間とは根本的に異なっているのです。

LLMがテキストを認識する単位、それが**「トークン」**です。

これを、料理に例えてみましょう。LLMは、文章という「長いレシピ」を直接は読めません。その代わりに、「トークナイザー」という非常に優秀な下ごしらえ係がいます。この係は、レシピ（文章）を受け取ると、まずそれを意味のある食材（トークン）ごとに、決まったルールで切り分けていきます。例えば、「東京都」という文字を見たら、「東京」と「都」という2つの食材に切り分ける、といった具合です。

そして、切り分けた食材（トークン）には、それぞれ「食材番号（トークンID）」が振られます。LLMの厨房では、この番号で管理された食材だけを扱って、次の料理（返答）を組み立てているのです。

この「トークン」という独特な世界の見方が、LLMの得意なこと、そして「苦手なこと」を生み出しています。ここからは、人間との3つの決定的な違いを見ていきましょう。

まず一つ目の違いは、LLMのトークナイザーが**「いつでも、誰が使っても、全く同じルールでしか動けない」という、融通の利かない性質**を持つ点です。人間は「こんちには」というタイプミスを見ても、文脈から「こんにちは」のことだろうな、と曖昧に認識できます。しかし、ルール通りにしか動けないLLMは、そうはいきません。

例えば、OpenAIのモデルで使われているトークナイザーでは、「こんにちは」は1つのトークンとして扱われるかもしれません。しかし、タイプミスである「こんちには」は、「こん」「ちに」「は」という3つの、全く異なるトークンに分解されてしまいます。LLMにとっては、これらは完全に別物。この厳密さゆえに、LLMは人間が見逃すような些細なタイプミスを簡単に見抜くことができますが、同時に、人間のような柔軟な解釈は苦手なのです。

二つ目の違いは、LLMは**一度トークンに分解してしまうと、その中身を細かく操作するのが非常に苦手**だという点です。

例えば、ChatGPTに「この文章の単語を、一つひとつ逆さまにして返してください」とお願いしたとします。単純なパターン操作なので得意そうに見えますよね。しかし、結果は見ての通り、めちゃくちゃです。正しく逆転できていません。なぜなら、「プロンプト」という単語を逆さまにするには、「プ-ロ-ン-プ-ト」と一文字ずつ認識し、並べ替える必要があります。しかし、LLMはこれを「プロンプト」という1つのトークン（塊）として見ているため、トークンを分解して、中の文字を並べ替える、という作業が非常に困難なのです。

ここから得られる教訓は、「**トークンを分解・再構築するようなタスクは、LLMに直接やらせない**」ということです。もしアプリケーションでそのような処理が必要な場合は、LLMに入力する前に前処理を行うか、LLMからの出力の後に後処理を行うように設計するのが賢明です。

例えば、「『あ』で始まる都道府県を教えて」とLLMに尋ねると、最初は「ありません」と答えたり、「青森県」は挙げても「愛知県」や「秋田県」を見落としたりします。これは、「愛知県」や「秋田県」がそれぞれ1つのトークンとして扱われているため、「あ」という文字で始まる、という文字レベルの条件判断が苦手だからです。

この場合の賢い使い方は、まずLLMに「日本の都道府県をすべてリストアップして」と大まかにお願いし、そのリストの中から「あ」で始まる都道府県を、私たちのプログラム側で探す、という分業です。

そして三つ目の違いは、**文字種の扱い**です。日本語には大文字・小文字の区別はありませんが、似たような例として全角文字と半角文字があります。人間にとって、全角の「Ａ」と半角の「A」は同じ文字の変形版に過ぎません。しかし、LLMにとっては、これらは全くの別人です。

ある小さなモデルに、全角のアルファベットをすべて半角にするよう依頼したところ、おかしな間違いが起きることがあります。例えば、「ＡＢＣ」は正しく「ABC」にできても、「ＸＹＺ」を変換しようとすると、おかしな記号になってしまう、といった具合です。

これもトークン化が原因です。LLMは、大量の学習データから「ABC」という半角のトークンの組み合わせはよく見るけれど、それが全角の「ＡＢＣ」とイコールだとは、すぐには結びつけられないのです。もちろん、最近の高性能なモデルはもっと上手に処理できますが、それでもモデルにとっては余計な負荷がかかります。単純な文字種の変換は、LLMに頼むまでもない単純作業です。賢明なプロンプトエンジニアは、LLMに無駄な労力を使わせることは避けるべきです。

さて、ここまでLLMが世界を「トークン」という単位で見ていること、そしてそれが原因でいくつかの弱点があることを学びました。では、なぜこの「トークン」という概念が、プロンプトエンジニアリングにおいてこれほど重要なのでしょうか？

それは、LLMが持つ、ある**絶対的な制約**に直結しているからです。それが、**「コンテキストウィンドウ」**です。

コンテキストウィンドウとは、一言でいえば、**「LLMが一度に覚えておける情報量の上限」**のことです。これを、**AIの「作業用の机の広さ」**に例えてみましょう。

私たちは、机の上に資料を広げて作業しますよね。机が広ければたくさんの資料を一度に見渡せますが、机が狭いと、新しい資料を置くためには古い資料をしまわなければなりません。LLMも全く同じで、この「机（コンテキストウィンドウ）」に収まりきらない情報は、忘れていってしまうのです。

LLMは、私たちが与えたプロンプトをこの机の上に広げ、それを参照しながら次に来るトークンを予測します。そして、生成したトークンも、またこの机の上に追加されていきます。しかし、この机のサイズには限りがあります。通常、数千から、最近のモデルでは数十万トークンといったサイズですが、それでも無限ではありません。

この机の上がトークンでいっぱいになってしまうと、LLMはそれ以上古い情報を参照できなくなり、文脈を見失ってしまいます。これが、非常に長い会話の途中で、AIが急に話の文脈を忘れてしまったり、矛盾したことを言い始めたりする原因です。

このコンテキストウィンドウという制約があるからこそ、私たちはプロンプトに含める情報を賢く取捨選択しなければなりません。限られたウィンドウの中に、問題解決に必要な情報を、いかに効率よく、かつ効果的に詰め込むか。これこそが、プロンプトエンジニアの腕の見せ所なのです。

それでは、今回のまとめです。今回は、LLMが私たち人間とは異なる方法で世界を見ていることを学びました。そのキーワードは「トークン」です。LLMはテキストをトークンという単位に分解して処理するため、文字レベルの操作が苦手などの弱点が生まれます。そして、このトークンという概念は、LLMの能力を制限する「コンテキストウィンドウ」という絶対的な制約につながっています。

このAIならではの世界の見方を理解することが、彼らと「共感」し、より良い関係を築くための第一歩です。

それでは、最後に「やってみよう！」のコーナーです。OpenAIが提供している「Tokenizer」というオンラインツールなどを使うと、好きな文章がどのようにトークンに分解されるかを確認できます。日本語と英語で、同じ意味の単語がどれくらいトークン数で違うのか、自分の名前はいくつのトークンになるのか、ぜひ試してみてください。LLMの世界が、より身近に感じられるはずです。

さて、次回は「LLMの文章生成術 〜AIの出力はこうして決まる〜」です。AIは、このトークンを、どのように組み立てて、あの自然な文章を生成しているのでしょうか？その創造性の源泉に迫ります。