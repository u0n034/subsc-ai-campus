
### 【第3回】LLMの文章生成術 〜AIの出力はこうして決まる〜

皆さん、こんにちは。「プロンプトエンジニアリング入門」第3回へようこそ。

前回、私たちはLLMが私たち人間とは異なり、世界を **「トークン」**という単位で見ていること、そしてその能力には **「コンテキストウィンドウ」** という絶対的な制約があることを学びました。

では、AIはこれらのトークンを、一体どのように組み合わせて、あの流暢で、時にはっとするほど創造的な文章を生み出しているのでしょうか？ 今回は、いよいよLLMの文章生成の核心、その創造性の源泉に迫ります。この仕組みを理解すれば、AIの出力をコントロールするための、より強力な武器が手に入ります。

LLMが文章を生成するプロセスは、私たちが文章を書くプロセスとは全く異なります。私たちは、まず頭の中に全体の構成を思い浮かべ、文章を書き進め、時には立ち止まって前の文章を修正したり、構成を練り直したりしますよね。

しかし、LLMはそうではありません。LLMの文章生成は、もっとシンプルで、そして後戻りの許されない、一方通行のプロセスなのです。この仕組みを **「自己回帰（オートリグレッシブ）モデル」** と呼びます。

自己回帰とは、文字通り「自分自身（過去の出力）を参照して、次の出力を決める」という仕組みです。LLMは、プロンプトとして与えられたトークンの並びを見て、統計的に最も可能性の高い「次に来るべき**たった1つ**のトークン」を予測します。そして、その予測したトークンを、元のトークンの列の末尾に追加します。今度は、新しく長くなったトークンの列全体を見て、また次のたった1つのトークンを予測する…。このプロセスを、延々と繰り返していくのです。

スマートフォンの予測変換をイメージしてみてください。キーボードの上に出てくる候補の中から、一つを選び続ける作業に似ています。LLMは、この作業を驚異的なスピードで、一文字ずつではなくトークン単位で行っているのです。

この「自己回帰」という仕組みから、LLMの非常に重要な特性が導き出されます。それは、**「一度トークンを出力したら、二度と後戻りして修正することはできない」**ということです。

LLMには、私たちが持つような「推敲」や「編集」という概念がありません。なぜなら、LLMが学習した完成された文章には、「あ、ここは間違えたから書き直そう」といった編集過程の記録はほとんど残っていないからです。そのため、LLMは一度出力した内容が間違いだと気づいても、それを自ら取り消すことはしないのです。

この「後戻りできない」性質は、時として奇妙な現象を引き起こします。その一つが「パターンの繰り返し」です。LLMはパターン認識が非常に得意なため、一度何らかのパターン、例えば箇条書きや特定の言い回しを始めると、それを延々と繰り返してしまうことがあります。なぜなら、各ステップにおいて、そのパターンを続けることが、統計的に最も「それらしい」と判断されてしまうからです。モデルは、人間のように「この辺でやめておこう」と飽きることはありません。

この特性を理解しておけば、AIがなぜ時々、頑固で不自然な振る舞いをするのかが腑に落ちるはずです。

さて、LLMは「次にくる最も可能性の高いトークン」を選ぶ、と説明しました。しかし、もし常に最も可能性の高い選択肢だけを選んでいたら、AIの答えはいつも同じで、退屈なものになってしまうでしょう。

実は、ここにAIの創造性をコントロールする鍵、**「temperature（テンパレチャー）」**という魔法のツマミが存在します。

LLMは、次にくるトークンを選ぶ際、実際には語彙に含まれる全てのトークンに対して「出現する確率」を計算しています。そして、どのトークンを選ぶかという「サンプリング」のプロセスで、このtemperatureというパラメータが活躍するのです。

temperatureは0以上の数値で、AIにどれだけ「創造的」になってほしいかを指示します。

* **temperatureが0に近い場合**：AIは、最も確率の高い、手堅い選択肢をほぼ必ず選びます。出力は非常に論理的で予測可能になります。正確性が求められる分類タスクや、事実に基づいた応答が欲しい場合に適しています。まるで、非常に忠実で真面目な秘書のようです。

* **temperatureが高い場合（例えば1.0など）**：AIは、確率が少し低いトークンも、あえて選ぶようになります。これにより、より多様で、予期せぬ、創造的な出力が生まれやすくなります。新しいアイデアを出してほしいブレインストーミングの場面などで活躍します。アイデア豊富なパートナーのようになってくれるでしょう。

しかし、このツマミを上げすぎると注意が必要です。temperatureが2.0のように極端に高くなると、LLMはまるで酔っぱらった詩人のように、意味の通らない、支離滅裂な文章を生成し始めます。これは、モデルが自ら生成した小さな誤りをパターンとして捉え、その誤りをさらに増幅させてしまうからです。

このように、私たちはtemperatureというパラメータを調整することで、AIの応答のランダム性、つまり創造性と論理性のバランスを自在にコントロールできるのです。

ここまで、LLMが1トークンずつ、確率的に文章を生成する仕組みを見てきました。では最後に、LLMの頭脳の内部、その驚くべきアーキテクチャを覗いてみましょう。現代のLLMのほとんどは、**「トランスフォーマー」**というアーキテクチャを基盤としています。

トランスフォーマーを理解する鍵は、それを一つの巨大な脳と考えるのではなく、**「無数の小さな脳（ミニブレイン）が集まったチーム」**だと想像することです。プロンプトに含まれるトークン一つひとつに、それぞれ担当のミニブレインが割り当てられています。

そして、これらのミニブレインたちは、**「アテンション（注意）機構」**と呼ばれる仕組みを使って、互いに情報を交換し、協力し合って文脈を理解しようとします。

アテンション機構は、Q&Aセッションのようなものです。あるミニブレインが「この『彼』というトークンは誰を指しているの？」と質問を投げかけると、別の場所にある「山田さん」というトークンを担当するミニブレインが「話し手は山田さんですよ」と答える。このようにして、トークン同士の関連性の強さを計算し、文章全体の意味を把握していくのです。

このトランスフォーマーとアテンション機構の仕組みから、プロンプトエンジニアリングにおける**絶対的な鉄則**が生まれます。

それは、**「情報の順序が、極めて重要である」**ということです。

なぜ順序が重要なのか。それは、アテンション機構には **「後方、つまり自分より前に出現したトークンしか見ることができない」**という厳密なルールがあるからです。LLMは、文章を前から後ろへと一度しか読みません。人間のように、後から前の部分を読み返して確認することはできないのです。

この制約が、いかに重要かを示す、面白い例があります。

ある段落の文章をChatGPTに見せた後で、「さて、今読んだ段落には何文字ありましたか？」と尋ねてみました。人間なら、もう一度段落を読み返して数えれば答えられますよね。しかし、ChatGPTの答えは、全くのでたらめでした。

なぜでしょう？ それは、ChatGPTのミニブレインたちが段落を読んでいる時点では、その後に「文字数を数えろ」という指示が来ることを全く知らないからです。彼らは、文の意味やスタイルといった他の情報に「アテンション（注意）」を向けており、後から重要になる「文字数」という情報には、全く注意を払っていなかったのです。

しかし、この質問を段落の**前**に持ってきて、「次の段落を読んで、文字数を数えてください」と指示した場合はどうでしょう。結果は、まだ完璧ではありませんでしたが、格段に正解に近い数値を返してきました。

この例が示す教訓は明確です。「**LLMに何かをさせたいなら、その指示やコンテキストは、関連する情報よりも先に提示しなければならない**」。これが、後戻りできないAIを正しく導くための、最も基本的で強力なテクニックなのです。

それでは、今回のまとめです。今回は、LLMの文章生成の裏側にある3つの重要な仕組みを学びました。

第一に、LLMは1トークンずつ文章を生成し、後戻りができない **「自己回帰モデル」**であること。
第二に、その創造性は **「temperature」**というパラメータで調整できること。
そして最後に、LLMの頭脳である **「トランスフォーマー」**は後方しか見られないため、プロンプトの **「情報の順序」**が決定的に重要であること。

LLMの能力を最大限に引き出すためには、私たちがLLMに「考えを声に出しながら前進する」ことを許し、正しい順序で思考の道筋を示してあげることが不可欠なのです。

さて、ここまででLLMの基本的な仕組みは、かなり理解できたのではないでしょうか。では、この基本原理を応用して、私たちが普段使っている「チャットAI」は、どのように作られているのでしょうか？

次回は、いよいよ **「『チャット』と『ツール』の仕組み 〜アシスタントへの進化〜」** と題して、現代のAIアシスタントを支える技術に迫ります。