### 【第8回】モデルの出力を制御する 〜望み通りの結果を得るコツ〜

皆さん、こんにちは。「プロンプトエンジニアリング入門」第8回へようこそ。

前回は、最高のプロンプトを組み立てるための設計図と、目的に応じた3つの代表的なドキュメント形式について学びました。最高の材料を、最高のレシピと構造で組み合わせる技術です。

さて、完璧なプロンプトという「最高の脚本」をAIに渡した今、私たちの仕事は終わりでしょうか？ いいえ、ここからがクライマックスです。優れた監督が、名優から最高の演技を引き出すように、私たちプロンプトエンジニアも、AIという役者の演技、つまり「出力」を細かく制御し、私たちが本当に望むシーンだけを切り取る技術が必要なのです。

今回は、モデルの出力をコントロールし、その思考をさらに深く読み解く、プロンプトエンジニアリングの仕上げの技術について学んでいきましょう。

まずは、モデルが生成する回答、つまり「補完」を解剖してみましょう。一見すると、ただの文章の塊に見えるAIの回答ですが、その構造を理解することが、制御の第一歩です。

モデルの補完は、大きく3つのパートに分けられます。

* **パート1：序文**
これは、本題の回答が始まる前の、前置きの部分です。「もちろんです！ご質問にお答えします」といった丁寧な挨拶や、モデルが思考を整理するための推論などが含まれます。

* **パート2：関連する解決策**
ここが、私たちが本当に欲しい、問題解決の核心となる「本題の回答」です。

* **パート3：追記**
そして、本題が終わった後に追加される、免責事項や関連情報などの「つまらない追記」です。

私たちが目指すのは、この中からパート2の「関連する解決策」だけを、いかに正確に、かつ効率的に取り出すか、ということです。そのために、邪魔になる可能性のある「序文」と「追記」を、うまくコントロールする必要があるのです。

特に問題となるのが、本題の前に出てくる「序文」と、本題の後に出てくる不要な「追記」です。今回は、この「序文」のコントロールに焦点を当ててみましょう。実は、すべての序文が悪者というわけではありません。

(画面に2種類の序文「邪魔な序文（無意味な要素）」と「有益な序文（推論）」が比較して表示される)

私たちが排除したいのは、「もちろんです！」「ご質問にお答えします」といった、丁寧ではあるもののプログラムでの自動処理には不要な **「無意味な要素（fluff）」**です。これらを制御するには、プロンプトのフォーマットを工夫するのが有効です。例えば、「1. 回答、2. 背景説明」のように、本題の回答とその他の要素を明確に分離するよう指示することで、AIはまず核心的な答えから話し始めるようになり、私たちはその部分だけを簡単に抽出できるのです。

一方で、私たちが積極的に生成させるべき、「有益な序文」も存在します。その代表格こそが、現代プロンプトエンジニアリングの最重要テクニックの一つ、「思考の連鎖（Chain-of-Thought）」、通称「CoT（シー・オー・ティー）」プロンプティングなのです。

「思考の連鎖」、一体どういうことでしょうか？ これを理解するために、まず人間の思考プロセスと、LLMの根本的な違いを思い出してみましょう。

私たちは、難しい数学の問題を解くとき、いきなり答えを書きませんよね。まず、問題文を読んで、どの公式を使うか考え、途中の計算式を紙に書き出し、ステップを踏んで、段階的に答えにたどり着きます。私たちには、頭の中で考えを巡らせたり、紙に書き出して思考を整理したりする **「内的な思考プロセス」**があります。

しかし、これまでの講座で学んだように、LLMにはこの「内的な思考プロセス」がありません。LLMは、後戻りのできない自己回帰モデルです。プロンプトを前から一度読むだけで、いきなり結論のトークンを生成し始めます。そのため、複雑な推論が必要な問題に対しては、最初の「直感的な推測」で間違った方向に進んでしまい、そのまま誤った答えを出してしまうことが多々あるのです。

そこで、この弱点を克服するために編み出されたのが、CoTプロンプティングです。これは、一言でいえば、**「LLMに、結論を出す前に、その思考プロセスそのものを文章として出力させる」**テクニックです。

人間に「途中の計算式も全部書きなさい」と指示するのと同じように、LLMにも「どうやってその結論に至ったのか、思考のステップを全部声に出して教えて」とお願いするのです。

では、なぜこれが効果的なのでしょうか？ それは、LLMが「自己回帰モデル」であるという性質を、逆手に取っているからです。

一度、LLMに「まずAとBを足してCを求め、次にそのCをDで割ります」といった思考プロセスをテキストとして出力させると、その文章自体が、LLMにとっての新しいコンテキストになります。次にLLMが最終的な答えのトークンを予測する際には、自らが出力したその論理的な思考プロセスと矛盾しないように、つまり、その思考の連鎖に沿った形で、答えを導き出そうとします。結果として、いきなり結論を出させるよりも、はるかに正答率が劇的に向上するのです。

では、具体的にどうやって使うのでしょうか？ 最も簡単で、かつ非常に強力なのが **「Zero-shot（ゼロショット） CoT」**と呼ばれる方法です。プロンプトの最後に、たった一言、魔法の言葉を付け加えるだけです。

そうです。「段階的に考えましょう」と付け加えるだけで、LLMは自発的に思考プロセスを書き出し始め、より正確な答えにたどり着くようになります。これは、複雑な論理パズルや計算問題を解かせたい時に、絶大な効果を発揮しますので、ぜひ覚えておいてください。

他にも、いくつかの思考プロセスを含んだQ&Aの例を見せる「Few-shot CoT」という方法もありますが、まずはこの「段階的に考えましょう」という魔法の言葉を使いこなせるようになることが重要です。

このように、CoTは、LLMの「序文」を意図的にコントロールし、モデルに深く考えさせることで、その推論能力を最大限に引き出す、非常に重要なテクニックなのです。

さて、回答の構造を理解した上で、次なる課題は「どうやって無駄な生成を止めさせるか」です。AIが必要な回答をすべて生成し終えた後も、延々と追記を書き続けてしまうと、それだけ時間とコストの無駄になってしまいます。

さて、有益な序文である「思考の連鎖」を生成させた上で、回答の「本題」が終わった後に続く、不要な追記をどう断ち切るか。ここで活躍するのが **「停止シーケンス（stop sequence）」**です。

停止シーケンスとは、その名の通り、「この文字列が出現したら、その時点でトークンの生成を停止してください」とAIに命令する、魔法の言葉です。

例えば、あなたがMarkdown形式でレポートを生成させているとしましょう。レポートの「# 結論」セクションを書き終えた後、モデルが勝手に次の「# 付録」セクションを書き始めてしまうのを防ぎたい。この場合、停止シーケンスとして、改行を含む「`\n#`」という文字列を指定しておきます。すると、モデルが結論を書き終え、次の見出しを作ろうと「改行してシャープ」というトークンを生成した瞬間に、ピタッと生成が停止するのです。

これにより、私たちは無駄なトークンにお金と時間を費やすことなく、必要な情報だけを確実に手に入れることができます。この停止シーケンスは、プログラムで出力を確実に解析するための、必須のテクニックと言えるでしょう。

ここまで、私たちはAIが出力する「テキスト」をどう制御するかを学んできました。しかし、実はLLMは、テキスト以外にも、非常に重要な情報を出力することができます。それが、各トークンが生成される **「確率」**です。本書では、この確率情報を活用するテクニックを **「logprobトリック」**と呼んでいます。

Logprobとは、「logarithmic probability」の略で、日本語では「対数確率」と訳されます。難しそうに聞こえますが、要は **「AIが、そのトークンを選んだことに対する『自信度』を示すスコア」**だと考えてください。このスコアを覗き見ることで、私たちはAIの思考プロセスを、より深く理解できるのです。

このlogprobトリックには、主に3つの強力な活用法があります。

* **活用法1：回答の品質チェック**
生成された回答全体のlogprobの平均値を見ることで、その回答に対するAIの全体的な自信度を測ることができます。もしAIが非常に自信なさげな（logprobが低い）回答をしてきたら、「この回答は不確かかもしれません」とユーザーに警告を出したり、より高性能なモデルで再試行させたり、といった判断が可能になります。

* **活用法2：確実な分類タスク**
「このメールは肯定的ですか、否定的ですか？」といった分類タスクをさせたい場合、「肯定的」「否定的」という単語を直接生成させるよりも、それぞれの単語が出現する「確率」を比較する方が、はるかに確実です。テキスト生成の曖昧さを排除し、モデルの「本心」を直接数値で比較できるのです。

* **活用法3：文章の重要部分の発見**
さらに面白いことに、logprobは、モデルからの回答だけでなく、私たちが入力した**プロンプト**自体に対しても計算させることができます。モデルがプロンプトを読んだ時に、「これは意外だ」と驚いた箇所、つまり統計的に出現確率が低いと判断した箇所は、logprobの値が著しく低くなります。その箇所は、文章のタイプミスであったり、あるいは、文脈の中で非常に重要な、情報密度の高いキーワードであったりする可能性が高いのです。この性質を使えば、長い文章の中から、AIが重要だと判断した部分を自動的にハイライトする、といった応用も考えられます。

さあ、これでプロンプトの作成から、出力の制御、そして分析まで、一連の技術を学びました。しかし、最後に一つ、すべてを決定づける、最も重要な選択が残っています。それは、**「どのモデルを使うか」**という選択です。

どんなに優れたプロンプトエンジニアリングを駆使しても、そもそもタスクに適していないモデルを選んでしまっては、望む結果は得られません。モデルの選択は、知能、スピード、コスト、そして機能といった、様々な要素のトレードオフを考える、複雑な意思決定です。

* **知能**を最優先するなら、最先端の大規模モデルを選ぶべきですが、**コスト**は高くなります。
* ユーザーとの対話で**スピード**が重要なら、小規模で高速なモデルが必要ですが、**知能**は少し犠牲になるかもしれません。

これらのトレードオフを理解し、自分のアプリケーションの要件に最も合致するモデルを見極めることが重要です。

さらに、既成のモデルを使うだけでなく、**「ファインチューニング」**という選択肢もあります。これは、既存のモデルを、特定のドメインやタスクに合わせて、追加でトレーニングする技術です。例えば、法律文書の扱いに特化したモデルや、自社の製品に関する質問にだけ答える社内用モデルなど、いわば「独自モデル」を作り上げることができるのです。

それでは、今回のまとめです。今回は、AIの出力をプロとして制御するための、仕上げの技術について学びました。

まず、モデルの回答を**「序文」「本題」「追記」**に解剖し、不要な部分をコントロールする方法を理解しました。そして、無駄な生成を断ち切る**「停止シーケンス」**の活用法。さらに、AIの自信度を覗き見る**「logprobトリック」**という強力な分析ツール。最後に、これら全ての土台となる、適切な**「モデル選択」**の重要性についても学びました。

これで、プロンプトエンジニアリングの「中心的なテクニック」に関するパートは全て終了です。皆さんは、LLMの仕組みを理解し、最高のプロンプトを組み立て、そしてその出力を精密にコントロールするための、一通りの知識と技術を身につけました。

しかし、プロンプトエンジニアリングの旅は、まだ終わりません。

次回は、いよいよ本講座の最終回。**「まとめと次のステップ：さらなる高みへ」**と題して、これまでの知識を総動員し、より高度な応用である「会話型エージェント」や「LLMワークフロー」、そしてプロンプトエンジニアリングの未来について、一緒に考えていきましょう。