### スライド構成案（AI時代の必須教養 5-1-2）

#### **スライド 1/23: タイトルと本日のテーマ**
*   **タイトル:** **【5-1-2】AIに潜むバイアスと公平性の問題**
*   **サブタイトル:** なぜAIは偏見を持ってしまうのか？
*   **ビジュアル:** 天秤が大きく傾いているイメージ。片方には多様な人々、もう片方には画一的なロボットのアイコンが乗り、ロボット側が重く沈んでいる。
*   **メッセージ:** AIは客観的で公平だと思っていませんか？

#### **スライド 2/23: 想像してみてください**
*   **タイトル:** **もしもあなたが...**
*   **内容:** 
    *   仕事応募 → AIが性別・出身地で自動不採用判定
    *   ローン申請 → AIが住居地域だけで「信用リスク高」と判断
*   **ビジュアル:** 困惑する人物の前に、「不採用」「リスク高」と冷たく表示されたデジタルのスクリーンが壁のように立ちはだかるイラスト。

#### **スライド 3/23: AIへの期待と現実のギャップ**
*   **タイトル:** **期待 vs 現実**
*   **ビジュアル:** 左側に輝くAIが人と握手している「期待」のイメージ。右側にAIが人に背を向け、その影が差別的な形（例：人を区別する壁）になっている「現実」のイラストを対比させる。
*   **期待:** 客観的で公平な判断
*   **現実:** 人間以上に残酷な差別の再生産・増幅

#### **スライド 4/23: 今日学ぶ3つのポイント**
*   **タイトル:** **AIバイアスを理解する3つのステップ**
*   **ビジュアル:** 「メカニズム」「社会的影響」「対策」の3つのステップを示すインフォグラフィック。それぞれに虫眼鏡、地球、道標のようなアイコンを添える。
*   **学習ポイント:**
    1. メカニズムの解明 - なぜAIは偏見を持つのか？
    2. 社会的影響の把握 - 公平性という価値の重要性
    3. 対策の探求 - 解決への道筋とガイドライン

#### **スライド 5/23: チャプター1**
*   **タイトル:** **第1章：AIバイアスとは何か？**
*   **ビジュアル:** 大きな「1」の数字と、AIの頭脳の内部（歯車や回路）を覗き込むようなイメージ。
*   **メッセージ:** バイアスの正体と発生メカニズムを探ります

#### **スライド 6/23: バイアスの二つの意味**
*   **タイトル:** **バイアスの二つの顔**
*   **ビジュアル:** 画面を左右に分割。左側に「統計学的バイアス」としてデータ分布の偏りを示すシンプルなグラフ。右側に「社会的バイアス」として色眼鏡をかけた人が他者を見ているイラスト。
*   **統計学的意味:** 「偏り」「誤差」（ニュートラル）
*   **社会的意味:** 「偏見」「先入観」（社会問題）

#### **スライド 7/23: AIとデータの関係**
*   **タイトル:** **データはAIの「教科書」**
*   **ビジュアル:** AIロボットが、歪んだ文字や偏った写真が載っている教科書を熱心に読んでいるイラスト。
*   **問題:** 教科書が偏っていたら、AIも偏った考え方を習得

#### **スライド 8/23: バイアス発生の3つの段階**
*   **タイトル:** **バイアスはどこで生まれるのか？**
*   **ビジュアル:** 「データ収集」→「アルゴリズム開発」→「人間からのフィードバック」というプロセスを示すインフォグラフィック。各段階で「歪み」のフィルターが追加され、最終的なアウトプットが大きく偏っていく様子を表現する。
*   **3つの段階:**
    1. データのバイアス - 学習データに含まれる偏り
    2. アルゴリズムのバイアス - 設計・開発過程での偏り
    3. 人間フィードバックのバイアス - 運用過程での偏り強化

#### **スライド 9/23: 原因① データのバイアス**
*   **タイトル:** **原因①：偏った教科書（データのバイアス）**
*   **ビジュアル:** 「医者=男性」「看護師=女性」といったステレオタイプなイラストが大量にデータとしてAIにインプットされるイメージ。
*   **問題:** 学習データに社会的不平等・歴史的偏見が反映
*   **例:** 「医者=男性」「看護師=女性」のステレオタイプを学習

#### **スライド 10/23: データの量的偏り**
*   **タイトル:** **見過ごされる少数派（データの量的偏り）**
*   **ビジュアル:** 顔認証システムのデモ画面。白人男性の顔は緑の枠で「認識成功 99%」と表示され、有色人種の女性の顔は赤の枠で「認識失敗 35%」と表示されている様子を並べて見せる。
*   **問題:** 特定グループのデータが極端に少ない
*   **実例:** 顔認証システム（白人男性：高認識率、有色人種女性：低認識率）

#### **スライド 11/23: 原因② アルゴリズムのバイアス**
*   **タイトル:** **原因②：意図せぬ誘導（アルゴリズムのバイアス）**
*   **ビジュアル:** ニュース推薦アルゴリズムの模式図。入力された多様なニュースが、アルゴリズムというブラックボックスを通過すると、扇情的で過激な記事ばかりが出口に集まってくる様子。
*   **問題:** 開発過程で意図しない偏りが組み込まれる
*   **例:** ニュース推薦で扇情的記事優先表示

#### **スライド 12/23: 原因③ 人間フィードバックのバイアス**
*   **タイトル:** **原因③：偏見の悪循環（人間フィードバック）**
*   **ビジュアル:** ユーザーが特定の情報ばかりをクリックし、その結果AIがますます偏った情報を提供するようになるループを、「AIの推薦→人間の選択→AIの学習」という矢印で表現した悪循環の図。
*   **問題:** ユーザーの無意識の偏見がAIに反映される悪循環

#### **スライド 13/23: チャプター2**
*   **タイトル:** **第2章：バイアスがもたらす社会的影響**
*   **ビジュアル:** 大きな「2」の数字と、AIによって引き起こされた社会問題（採用差別、ローン拒否、誤認逮捕など）を象徴するシーンのコラージュ写真。
*   **メッセージ:** 具体的事例から深刻な社会的影響を見ていきます

#### **スライド 14/23: 事例① Amazon AI採用ツール**
*   **タイトル:** **事例①：女性を差別した採用AI**
*   **ビジュアル:** Amazonのロゴと、大量の履歴書がベルトコンベアでAIに審査され、「女性」と関連するキーワードが含まれるものだけがシュレッダーにかけられていく風刺画。
*   **2018年** 問題発覚
*   **問題:** 「女性」含む履歴書を能力無関係で減点
*   **結果:** プロジェクト中止

#### **スライド 15/23: 事例② 金融における信用スコア差別**
*   **タイトル:** **事例②：住む場所で人生が決まる？**
*   **ビジュアル:** 地図上で特定の地域だけが赤く塗られ、その地域に住む人々がローンや教育の機会から締め出されている（見えない壁で隔てられている）イメージ図。
*   **問題:** 居住地・人種で信用評価
*   **影響:** 教育・住宅機会剥奪 → 社会格差固定化

#### **スライド 16/23: 事例③ 司法分野のCOMPAS**
*   **タイトル:** **事例③：AIは公正な裁判官か？**
*   **ビジュアル:** 白人被告人と黒人被告人の上に、「再犯リスク」を示すゲージが表示されている。能力や経歴が似ているにも関わらず、黒人被告人のゲージが不当に高く表示されているイラスト。背景に裁判所のシルエット。
*   **用途:** 再犯リスク予測（保釈・量刑に影響）
*   **問題:** 黒人被告人により高いリスク予測

#### **スライド 17/23: AIバイアスの本質**
*   **タイトル:** **これは「技術エラー」ではない**
*   **ビジュアル:** AIの顔をしたロボットが、「機会平等」「公正」「人権」と書かれた本を踏みにじっている、インパクトのあるイラスト。
*   **本質:** 技術的エラーではなく重大な社会問題
*   **脅威:** 機会平等・社会的公正・基本的人権を侵害

#### **スライド 18/23: 公平性の複雑さ**
*   **タイトル:** **何が「公平」なのか？**
*   **ビジュアル:** 身長の違う3人がフェンス越しに野球を見ている有名なイラスト。左側に「形式的平等」として全員に同じ高さの台を与え、結果的に背の低い人が見えていない様子。右側に「実質的平等」として身長に合わせて台の高さを調整し、全員が見えるようにした様子を対比させる。
*   **形式的平等:** 全員を同じように扱う
*   **実質的平等:** 不利なグループを積極支援
*   **課題:** 文脈により適切な公平性は異なる

#### **スライド 19/23: チャプター3**
*   **タイトル:** **第3章：バイアスへの挑戦 - 対策とガイドライン**
*   **ビジュアル:** 大きな「3」の数字と、エンジニア、倫理学者、法律家、市民など多様な人々が円卓でAIの未来について議論しているイメージ。
*   **アプローチ:** 技術・組織・制度の多層的対策

#### **スライド 20/23: 対策① 技術的アプローチ**
*   **タイトル:** **対策①：技術で偏りを正す**
*   **ビジュアル:** 3つのアイコンを横に並べる。1. バランスの取れたデータセットを示す天秤のアイコン。2. アルゴリズムを示す歯車に「公平性」のチェックマークが入っているアイコン。3. AIの頭脳が透明になり中身が見える「説明可能AI」のアイコン。
*   **データセット見直し** - 偏り特定・バランス改善
*   **アルゴリズム工夫** - 公平性制約・指標組み込み
*   **説明可能AI** - 判断根拠可視化

#### **スライド 21/23: 対策② 組織的アプローチ**
*   **タイトル:** **対策②：作る側の責任**
*   **ビジュアル:** 3つのアイコンを横に並べる。1. 多様な人種・性別の人物アイコンが集まっている開発チームのイラスト。2. 企業ロゴの横に掲げられた「AI倫理ガイドライン」の巻物。3. 開発プロセスの各段階に「倫理チェック」のスタンプが押されているイラスト。
*   **開発チーム多様性** - 様々な背景の確保
*   **AI倫理ガイドライン** - 企業レベルでの策定
*   **AIガバナンス** - 各段階での倫理チェック

#### **スライド 22/23: 対策③ 制度的アプローチ**
*   **タイトル:** **対策③：社会のルール作り**
*   **ビジュアル:** EU、OECD、日本の国旗と、それぞれの法律や原則を象徴するアイコン（法律の天秤、協力の握手、人間中心の円）を並べて表示する。
*   **EU AI法案** - 包括的規制
*   **OECD AI原則** - 人間中心・公平性提唱
*   **日本政府指針** - 人間中心のAI社会原則

#### **スライド 23/23: まとめと次回予告**
*   **タイトル:** **まとめ：AIは社会を映す鏡**
*   **ビジュアル:** AIという大きな鏡に、私たちの社会の姿（多様な人々が協力し合う理想の姿）が映っているイラスト。
*   **まとめ:** AIは社会を映す鏡
*   **3つのポイント:** 発生原因・社会的影響・対策アプローチ
*   **次回:** ブラックボックス問題と透明性・説明可能性 