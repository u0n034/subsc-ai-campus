### スライド構成案（AI時代の必須教養 5-1-3）

#### **スライド 1/22: タイトルと本日のテーマ**
*   **タイトル:** **【5-1-3】ブラックボックス問題と透明性・説明可能性(XAI)**
*   **サブタイトル:** なぜAIの判断は理解できないのか？
*   **ビジュアル:** 完全に閉ざされた黒い箱と困惑する人々
*   **メッセージ:** AIの「なぜ？」に答えることはできるのか

#### **スライド 2/22: AIドクターの診断**
*   **タイトル:** **もしもAIドクターが...**
*   **シナリオ:** 
    *   AIドクター：「99%の確率で深刻な病気です。すぐに治療を」
    *   あなた：「なぜですか？」
    *   AIドクター：「理由は説明できません」
*   **問いかけ:** この診断を信頼できますか？

#### **スライド 3/22: ブラックボックスとは**
*   **タイトル:** **「ブラックボックス」の定義**
*   **定義:** 入力と出力は分かるが、間の処理プロセスが見えず理解できない状態
*   **現実:** 現代の高性能AIの多くがブラックボックス状態
*   **問題:** 社会実装における大きな障壁

#### **スライド 4/22: 今日学ぶ3つのポイント**
*   **タイトル:** **ブラックボックス問題を理解する3ステップ**
*   **学習ポイント:**
    1. なぜブラックボックスになるのか？（構造的原因）
    2. なぜ問題なのか？（透明性と説明責任の重要性）
    3. どう解決するのか？（説明可能AI：XAI）

#### **スライド 5/22: チャプター1**
*   **タイトル:** **第1章：なぜAIの判断は「ブラックボックス」なのか？**
*   **メッセージ:** AIの驚異的性能を実現する構造的特性に原因あり

#### **スライド 6/22: 原因① モデルの圧倒的複雑性**
*   **タイトル:** **原因①：天文学的な数のパラメータ**
*   **構造:** ディープニューラルネットワーク = 多層の複雑ネットワーク
*   **規模例:** GPT-3 = 1750億個のパラメータ（銀河系の星の数に匹敵）
*   **問題:** 人間には理解不可能な複雑さ

#### **スライド 7/22: パラメータの自動調整**
*   **タイトル:** **膨大なパラメータが自動で最適化される**
*   **プロセス:** 学習でパラメータを少しずつ自動調整
*   **結果:** 天文学的パラメータの複雑な相互作用
*   **限界:** 人間が全体ロジックを理解することは不可能

#### **スライド 8/22: 原因② 特徴表現学習**
*   **タイトル:** **原因②：AIが独自の「見方」を学習**
*   **従来:** 人間が「特徴量」を設計（駅距離、広さなど）
*   **ディープラーニング:** AIが特徴量を自動発見
*   **問題:** AI独自の特徴は人間に理解不能

#### **スライド 9/22: 階層的特徴学習**
*   **タイトル:** **画像認識における階層的学習**
*   **学習プロセス:**
    *   浅い層：「線」「角」（単純な特徴）
    *   中間層：「目」「鼻」（複雑な特徴）
    *   深い層：「人間の顔」（抽象的概念）
*   **課題:** 人間には思いつかない微細な特徴を重視

#### **スライド 10/22: 解読不能な特徴**
*   **タイトル:** **AIの「説明」は数値の羅列**
*   **問題:** 
    *   AI独自の特徴量は人間の概念と不一致
    *   「なぜ猫と判断？」→ 解読不能な数値羅列
*   **結果:** 強力だがミステリアスなブラックボックス

#### **スライド 11/22: チャプター2**
*   **タイトル:** **第2章：なぜ「透明性」と「説明責任」が必要なのか**
*   **メッセージ:** ブラックボックス性が引き起こす深刻な問題

#### **スライド 12/22: 問題① 信頼性の欠如**
*   **タイトル:** **理由不明の判断は信頼されない**
*   **重要分野:** 
    *   金融機関の融資審査
    *   裁判における判決補助
    *   医療診断
*   **必要条件:** 判断理由の説明可能性

#### **スライド 13/22: 問題② 安全性の確保困難**
*   **タイトル:** **事故原因が特定できない**
*   **事例:** 自動運転車の事故
*   **問題:** 
    *   なぜその判断をしたか不明
    *   原因特定困難 → 改善・再発防止不可能
*   **必要性:** 安全性検証には透明性が不可欠

#### **スライド 14/22: 問題③ バイアス発見困難**
*   **タイトル:** **隠されたバイアスを発見できない**
*   **問題:** 
    *   差別的判断の根拠がブラックボックス内
    *   バイアス存在に気づけない
    *   原因特定・修正が困難
*   **必要性:** 問題解決には判断プロセスの透明性が必須

#### **スライド 15/22: 透明性とアカウンタビリティ**
*   **タイトル:** **AI倫理の重要原則**
*   **透明性:** AIシステムの内部構造・データ・アルゴリズムが理解可能
*   **アカウンタビリティ:** AI結果について合理的説明を提供する責任
*   **位置づけ:** 国内外のAI倫理ガイドラインで重要原則として明記

#### **スライド 16/22: チャプター3**
*   **タイトル:** **第3章：ブラックボックスを開ける試み - 説明可能AI（XAI）**
*   **XAI定義:** AIの判断根拠を人間に理解可能な形で提示する技術の総称

#### **スライド 17/22: XAI手法① 可視化技術**
*   **タイトル:** **画像認識での可視化：CAMとGrad-CAM**
*   **技術:** CNNが注目した画像部分をヒートマップで表示
*   **例:** AI「犬」認識 → 犬の顔・耳部分が赤くハイライト
*   **効果:** 判断根拠の直感的理解、モデルの信頼性検証

#### **スライド 18/22: XAI手法② LIME**
*   **タイトル:** **局所的説明：LIME（ライム）**
*   **特徴:** 特定のAIモデルに依存しない汎用手法
*   **動作:** 入力データに変化を加えて予測の変化を分析
*   **例:** 「顧客解約予測」→「利用頻度低下」「問い合わせ増加」が主要因

#### **スライド 19/22: XAI手法③ SHAP**
*   **タイトル:** **公平な貢献度分析：SHAP（シャップ）**
*   **理論基盤:** 協力ゲーム理論を応用
*   **考え方:** 各特徴量のチーム貢献度を公平に分配
*   **出力:** 全特徴量のポジティブ・ネガティブ貢献度を数値化

#### **スライド 20/22: XAIの現状と課題**
*   **タイトル:** **XAIは発展途上の分野**
*   **成果:** ブラックボックスの扉を少しずつ開けることに成功
*   **課題:** 
    *   AIの「説明」の正確性
    *   人間による解釈の妥当性
    *   新たな技術的・社会的問題

#### **スライド 21/22: AIとの新しい協働関係**
*   **タイトル:** **思考停止ではなく対話を**
*   **従来の危険な関係:** AIに全てを任せて思考停止
*   **目指すべき関係:** 
    *   AIと対話し判断を吟味
    *   最終責任は人間が負う
    *   XAIがコミュニケーションツール

#### **スライド 22/22: まとめと次回予告**
*   **タイトル:** **まとめ：ブラックボックスを開けて信頼を築く**
*   **3つのポイント:**
    1. ブラックボックス原因：複雑性と特徴表現学習
    2. 透明性・説明責任：信頼性・安全性・バイアス発見に不可欠
    3. XAI技術：CAM、LIME、SHAPで判断根拠を可視化
*   **次回:** ディープフェイクとフェイクニュース - AIの意図的悪用 