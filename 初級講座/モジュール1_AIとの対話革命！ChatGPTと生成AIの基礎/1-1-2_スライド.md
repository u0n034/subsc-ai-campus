---
slide: 1
---

# 第2部：革命前夜
## 生成AIを可能にした技術的ブレークスルー

---
slide: 2
---

## 従来型AIの仕組み
### 3つの主要な学習アプローチ

---
slide: 3
---

### 教師あり学習
#### 正解データからパターンを学ぶ

*   AIに「問題」と「正解」のペアを与え、法則を学習させる。
*   身近な例：スパムメールフィルター

---
slide: 4
---

### 教師なし学習
#### データから構造を自律的に発見

*   正解ラベルなしのデータから、AIが構造やパターンを見つけ出す。
*   応用例：顧客のグループ分け（クラスタリング）

---
slide: 5
---

### 強化学習
#### 試行錯誤で最適解を学ぶ

*   環境内で試行錯誤し、「報酬」を最大化する行動を学習する。
*   代表例：囲碁AI「AlphaGo」

---
slide: 6
---

### AlphaGoの衝撃

**2016年**、囲碁の世界王者に勝利。
*   人間の棋譜と天文学的な自己対戦で学習。
*   人間の固定観念を超える「神の一手」を獲得。

---
slide: 7
---

## Transformerの登場
### "Attention is All You Need"

---
slide: 8
---

### これまでの言語モデルの「壁」

*   課題は「長い文脈の理解」でした。
*   従来のモデルは、文頭の情報を「忘れてしまう」という構造的欠点があった。

---
slide: 9
---

### なぜ「忘れる」のか？

> 「…**猫**は、…ひどく疲れていたので、…毛布の上で**丸くなった**」

文末「丸くなった」の主語「猫」との距離が離れすぎており、関連付けが困難でした。

---
slide: 10
---

### 革命の号砲：「Attention Is All You Need」

**2017年**、Googleの研究者が歴史的論文を発表。
*   現在全てのLLMの心臓部である**Transformer**を提案。
*   その核心技術が「Self-Attention」機構。

---
slide: 11
---

### Transformerの核心：Self-Attention

*   文章を順番に処理せず、全単語の関連性を一気に計算する。
*   単語間の距離に関係なく、文脈を正確に捉えることが可能になった。

---
slide: 12
---

## LLMと拡散モデル
### 知性と創造性の源泉

---
slide: 13
---

### 1. 大規模言語モデル (LLM)

*   Transformerをベースにモデルとデータ量を巨大化。
*   「量」の増加が「質」へ転化する**スケーリング則**を発見。
*   訓練されていない能力が自発的に現れる「創発」現象が起きた。

---
slide: 14
---

### 2. 拡散モデル (Diffusion Models)

*   画像生成の世界で起きたパラダイムシフト。
*   創造のプロセスを「破壊」と「復元」から学ぶという独創的なアイデア。

---
slide: 15
---

### 拡散モデルの2ステップ

*   **① 破壊 (拡散過程)**
    *   画像にノイズを加え、完全なノイズにする。
    *   AIは「加えられたノイズ」のパターンを学習。

*   **② 復元 (逆拡散過程)**
    *   ノイズから、学習を元にノイズを除去していく。
    *   全く新しい高品質な画像を「創造」する。

---
slide: 16
---

### まとめ：革命前夜、技術は揃った

*   **Transformer**: 言語の深い理解を実現。
*   **LLMと創発**: 汎用的な知性への道筋。
*   **拡散モデル**: 無からの創造を可能に。

---
slide: 17
---

### 革命の、前夜でした。

これらの技術が融合し、AIはついに
人間の知的活動の根幹であった
**「言語」と「創造性」**の領域へ。 